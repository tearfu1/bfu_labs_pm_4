{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQO45sPGAsfG",
        "outputId": "1a8023cc-5976-4a08-8d97-d9b48c8604e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Текст загружен. Длина: 692221 симв.\n",
            "Уникальных символов: 151\n",
            "Начинаю обучение...\n",
            "Epoch 1/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 3.5539\n",
            "Epoch 1: saving model to ./training_checkpoints/ckpt_1.weights.h5\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 83ms/step - loss: 3.5495\n",
            "Epoch 2/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 2.5899\n",
            "Epoch 2: saving model to ./training_checkpoints/ckpt_2.weights.h5\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 85ms/step - loss: 2.5895\n",
            "Epoch 3/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 2.4179\n",
            "Epoch 3: saving model to ./training_checkpoints/ckpt_3.weights.h5\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 87ms/step - loss: 2.4176\n",
            "Epoch 4/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 2.2827\n",
            "Epoch 4: saving model to ./training_checkpoints/ckpt_4.weights.h5\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - loss: 2.2824\n",
            "Epoch 5/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 2.1614\n",
            "Epoch 5: saving model to ./training_checkpoints/ckpt_5.weights.h5\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - loss: 2.1611\n",
            "Epoch 6/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 2.0595\n",
            "Epoch 6: saving model to ./training_checkpoints/ckpt_6.weights.h5\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - loss: 2.0593\n",
            "Epoch 7/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 1.9661\n",
            "Epoch 7: saving model to ./training_checkpoints/ckpt_7.weights.h5\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 81ms/step - loss: 1.9660\n",
            "Epoch 8/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 1.8944\n",
            "Epoch 8: saving model to ./training_checkpoints/ckpt_8.weights.h5\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - loss: 1.8942\n",
            "Epoch 9/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 1.8249\n",
            "Epoch 9: saving model to ./training_checkpoints/ckpt_9.weights.h5\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - loss: 1.8248\n",
            "Epoch 10/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 1.7726\n",
            "Epoch 10: saving model to ./training_checkpoints/ckpt_10.weights.h5\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - loss: 1.7725\n",
            "Epoch 11/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1.7240\n",
            "Epoch 11: saving model to ./training_checkpoints/ckpt_11.weights.h5\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - loss: 1.7240\n",
            "Epoch 12/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 1.6888\n",
            "Epoch 12: saving model to ./training_checkpoints/ckpt_12.weights.h5\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - loss: 1.6888\n",
            "Epoch 13/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 1.6496\n",
            "Epoch 13: saving model to ./training_checkpoints/ckpt_13.weights.h5\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 83ms/step - loss: 1.6496\n",
            "Epoch 14/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 1.6161\n",
            "Epoch 14: saving model to ./training_checkpoints/ckpt_14.weights.h5\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - loss: 1.6162\n",
            "Epoch 15/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 1.5858\n",
            "Epoch 15: saving model to ./training_checkpoints/ckpt_15.weights.h5\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - loss: 1.5858\n",
            "Epoch 16/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 1.5585\n",
            "Epoch 16: saving model to ./training_checkpoints/ckpt_16.weights.h5\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - loss: 1.5585\n",
            "Epoch 17/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 1.5345\n",
            "Epoch 17: saving model to ./training_checkpoints/ckpt_17.weights.h5\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - loss: 1.5345\n",
            "Epoch 18/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 1.5076\n",
            "Epoch 18: saving model to ./training_checkpoints/ckpt_18.weights.h5\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - loss: 1.5076\n",
            "Epoch 19/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 1.4814\n",
            "Epoch 19: saving model to ./training_checkpoints/ckpt_19.weights.h5\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 83ms/step - loss: 1.4815\n",
            "Epoch 20/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 1.4598\n",
            "Epoch 20: saving model to ./training_checkpoints/ckpt_20.weights.h5\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - loss: 1.4598\n",
            "Генерация 1000 символов...\n",
            "\n",
            "==================================================\n",
            "РЕЗУЛЬТАТ ГЕНЕРАЦИИ:\n",
            "==================================================\n",
            "Читатели одно и потом – спосади, но только одна деревья, и я скачительно совершенно не слышит, только на меня установленное страницы – самое неизвестные носы. И все это – и я не могу сердце, что и другой стола (как вот сейчас же слова – и мы на кресла, не могу… – Все время – просто что я знаю, что я как же и такое уже почему, что его принять – я не могу не настолько опасно, вся нежные, голосами не видал оброзом века.\n",
            "\n",
            "Раз – потому что вам должно быть один кипнуть себе на городе, с крошечное, потому что-то ведь не знаю, когда отделить бы этим раз другой, потому что самое главное всего слова с какого-то дело минуту – и и все-таки не надо самые потому что же, не поверхно противнества в полу бы не надо подумал один раз они – так как неизлечие в бокрам, потому что с бесконечному и точно так и не было.\n",
            "\n",
            "– Ну, минуты, вы такие со смерть – и вот сейчас побежал как будто там с законом и остановился. То монашек страшительное – как кладка о розовом столиков Единого Государства. И построил сегодня был на ч\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "class MyLSTMModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "        super(MyLSTMModel, self).__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(rnn_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       stateful=True)\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, training=None, initial_state=None):\n",
        "        x = self.embedding(inputs)\n",
        "\n",
        "        x, h, c = self.lstm(x, initial_state=initial_state)\n",
        "\n",
        "        output = self.dense(x)\n",
        "\n",
        "        if training:\n",
        "            return output\n",
        "        return output, [h, c]\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.lstm.reset_states()\n",
        "\n",
        "class TextProcessor:\n",
        "    def __init__(self, filepath, encoding='utf-8', seq_length=100):\n",
        "        self.text = self._load_data(filepath, encoding)\n",
        "        self.vocab = sorted(list(set(self.text)))\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "        self.char2idx = {u: i for i, u in enumerate(self.vocab)}\n",
        "        self.idx2char = np.array(self.vocab)\n",
        "\n",
        "        print(f'Текст загружен. Длина: {len(self.text)} симв.')\n",
        "        print(f'Уникальных символов: {self.vocab_size}')\n",
        "\n",
        "    def _load_data(self, path, encoding):\n",
        "        with open(path, 'r', encoding=encoding) as f:\n",
        "            return f.read()\n",
        "\n",
        "    def create_dataset(self, batch_size=64, buffer_size=10000):\n",
        "        text_as_int = np.array([self.char2idx[c] for c in self.text])\n",
        "        char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "        sequences = char_dataset.batch(self.seq_length + 1, drop_remainder=True)\n",
        "\n",
        "        def split_input_target(chunk):\n",
        "            return chunk[:-1], chunk[1:]\n",
        "\n",
        "        dataset = sequences.map(split_input_target)\n",
        "        dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
        "\n",
        "        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "        return dataset\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, processor):\n",
        "        self.processor = processor\n",
        "        self.model = None\n",
        "        self.embedding_dim = 256\n",
        "        self.rnn_units = 1024\n",
        "        self.checkpoint_dir = './training_checkpoints'\n",
        "\n",
        "        if not os.path.exists(self.checkpoint_dir):\n",
        "            os.makedirs(self.checkpoint_dir)\n",
        "\n",
        "    def compile_model(self, batch_size):\n",
        "        model = MyLSTMModel(\n",
        "            vocab_size=self.processor.vocab_size,\n",
        "            embedding_dim=self.embedding_dim,\n",
        "            rnn_units=self.rnn_units,\n",
        "            batch_size=batch_size\n",
        "        )\n",
        "\n",
        "        dummy_input = tf.zeros((batch_size, 1), dtype=tf.int32)\n",
        "        model(dummy_input, training=False)\n",
        "\n",
        "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "        model.compile(optimizer='adam', loss=loss)\n",
        "        return model\n",
        "\n",
        "    def train(self, dataset, epochs=10):\n",
        "        self.model = self.compile_model(batch_size=64)\n",
        "        print(\"Начинаю обучение...\")\n",
        "\n",
        "        checkpoint_path = os.path.join(self.checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n",
        "        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "            filepath=checkpoint_path,\n",
        "            save_weights_only=True,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        history = self.model.fit(dataset, epochs=epochs, callbacks=[checkpoint_callback])\n",
        "        return history\n",
        "\n",
        "    def generate_text(self, start_string, num_generate=1000, temperature=1.0):\n",
        "        gen_model = self.compile_model(batch_size=1)\n",
        "\n",
        "        gen_model.set_weights(self.model.get_weights())\n",
        "\n",
        "        gen_model.reset_states()\n",
        "\n",
        "        input_eval = [self.processor.char2idx[s] for s in start_string]\n",
        "        input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "        text_generated = []\n",
        "        states = None\n",
        "\n",
        "        print(f\"Генерация {num_generate} символов...\")\n",
        "        for i in range(num_generate):\n",
        "            predictions, states = gen_model(input_eval, training=False, initial_state=states)\n",
        "\n",
        "            predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "            predictions = predictions / temperature\n",
        "\n",
        "            predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "            input_eval = tf.expand_dims([predicted_id], 0)\n",
        "            text_generated.append(self.processor.idx2char[predicted_id])\n",
        "\n",
        "        return start_string + ''.join(text_generated)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        proc = TextProcessor('input.txt', encoding='utf-8')\n",
        "        train_data = proc.create_dataset(batch_size=64)\n",
        "\n",
        "        agent = Agent(proc)\n",
        "\n",
        "        agent.train(train_data, epochs=20)\n",
        "\n",
        "        result = agent.generate_text(start_string=u\"Читатели \", num_generate=1000, temperature=0.6)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"РЕЗУЛЬТАТ ГЕНЕРАЦИИ:\")\n",
        "        print(\"=\"*50)\n",
        "        print(result)\n",
        "\n",
        "        # Сохранение\n",
        "        with open('generated_result.txt', 'w', encoding='utf-8') as f:\n",
        "            f.write(result)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"error\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3G6Vfhgh93lo",
        "outputId": "f2173058-e48b-44a9-f159-0a431425c877"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Доступные GPU: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Доступные GPU:\", tf.config.list_physical_devices('GPU'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
